{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cataloguing the data\n",
    "\n",
    "## Example 1: Forced photometry and producing catalogues\n",
    "\n",
    "## Example 2: Running the Data pipeline\n",
    "\n",
    "There is one last class method for the Data object that we havn't quite covered yet, `Data.pipeline()` which again just takes `survey` and `version` inputs. This class method is what is used in the EPOCHS pipeline and essentially just chains the cataloguing steps in the previous notebooks together elegantly, skipping those that have already been executed in the past. For further details, please read the previous notebooks in this section if you have not already done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from galfind import Data\n",
    "\n",
    "survey = \"JOF\"\n",
    "version = \"v11\"\n",
    "\n",
    "# load the data object (short version)\n",
    "data_short = Data.pipeline(survey, version)\n",
    "\n",
    "# load the data object (long version)\n",
    "data_long = Data.from_survey_version(survey, version)\n",
    "data_long.PSF_homogenize(\"F444W\")\n",
    "data_long.segment(\"sextractor\")\n",
    "data_long.forced_photometry([\"F277W\", \"F356W\", \"F444W\"])\n",
    "data_long.mask()\n",
    "data_long.run_depths()\n",
    "\n",
    "# ensure the two data objects are the same\n",
    "assert data_short == data_long\n",
    "\n",
    "# show the data object attributes\n",
    "print(data_short)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
